{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function form Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Set of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 2.2, 1.1, 5],     # sample of 4 inputs\n",
    "     [0.1, 4, 0.9, 3], \n",
    "     [4, 2.1, 0.5, 3.2]]\n",
    "\n",
    "y = np.array([0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function to create a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):          \n",
    "        # weights matrix is of n_inputs * n_neurons\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        # biases will of same size of neurons we have\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # final output is going to be as (weights * inputs) + bias\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # it helps in removing the negative values\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we removing -ve values in ReLU but this will cause the problem of dying neuron.\n",
    "#### For -ve value it gives 0 so if we pass 0 to next neuron it becomes dead.\n",
    "## Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # exponentiating the inputs to get rid off -ve values\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        # normalizing them to keep their meaning \n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Layer 1 and applying ReLU Activation Function</h4>\n",
    "This layer has 4 input features and 5 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.11265603, 0.        , 0.14914444, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.4312359 , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.24134714, 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = Layer_Dense(4, 5)\n",
    "activation1 = Activation_ReLU()\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "activation1.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Layer 2 and applying Softmax Activation Function \n",
    "<p>Output of Layer 1 is input to Layer 2</p>\n",
    "So, it'll have 5 input and any number of neurons we can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55970493, 0.44029507],\n",
       "       [0.51868212, 0.48131788],\n",
       "       [0.52722099, 0.47277901]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2 = Layer_Dense(5, 2)\n",
    "activation2 = Activation_Softmax()\n",
    "layer2.forward(layer1.output)\n",
    "layer2.output\n",
    "activation2.forward(layer2.output)\n",
    "activation2.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function - \n",
    "### Categorical-Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, y_hat, y):\n",
    "        sample_loss = self.forward(y_hat, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Categorical_Cross_Entropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        # bound the predicted values to a specific range to handle numerical stability concerns\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # if scaler target value\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(samples), y_true]\n",
    "        # if one hot encodded target value\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidence)\n",
    "\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  0.6869000387125329\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_Categorical_Cross_Entropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(\"Loss : \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for this sample is : 0.6869\n",
    "<h4>On the basis of calculates loss the backpropagation is performed.</h4>\n",
    "<p>If loss is very high then the weigth can be adjusted by larger values as we are far from target.</p> \n",
    "<p>If loss is very low then the weight are adjusted by smaller values as we are nearer to target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
